{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Noor-Z1/Machine-Learning/blob/main/Decision_Tree_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQmHybnIt2K-"
      },
      "outputs": [],
      "source": [
        "\n",
        "from math import log2 as log\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# In the decision tree, non-leaf nodes are going to be represented via TreeNode\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self, attribute):\n",
        "        self.attribute = attribute\n",
        "        # dictionary, k: subtree, key (k) an attribute value, value is either TreeNode or TreeLeafNode\n",
        "        self.subtrees = {}\n",
        "\n",
        "\n",
        "# In the decision tree, leaf nodes are going to be represented via TreeLeafNode\n",
        "class TreeLeafNode:\n",
        "    def __init__(self, data, label):\n",
        "        self.data = data\n",
        "        self.labels = label\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, dataset: list, labels, features, criterion=\"information gain\"):\n",
        "        \"\"\"\n",
        "        :param dataset: array of data instances, each data instance is represented via an Python array\n",
        "        :param labels: array of the labels of the data instances\n",
        "        :param features: the array that stores the name of each feature dimension\n",
        "        :param criterion: depending on which criterion (\"information gain\" or \"gain ratio\") the splits are to be performed\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.labels = labels\n",
        "        self.features = features\n",
        "        self.criterion = criterion\n",
        "        # it keeps the root node of the decision tree\n",
        "        self.root = None\n",
        "\n",
        "        # further variables and functions can be added...\n",
        "\n",
        "\n",
        "\n",
        "    def calculate_entropy__(self, dataset, labels):\n",
        "        \"\"\"\n",
        "        :param dataset: array of the data instances\n",
        "        :param labels: array of the labels of the data instances\n",
        "        :return: calculated entropy value for the given dataset\n",
        "        \"\"\"\n",
        "        entropy_value = 0.0\n",
        "\n",
        "        pos_count = 0\n",
        "        neg_count = 0\n",
        "        \"\"\"\n",
        "        Entropy calculations\n",
        "        \"\"\"\n",
        "        #check if this is okay - that is do we need to make any mod if we want it to be more general entropy func\n",
        "\n",
        "        ins_num = len(labels)\n",
        "\n",
        "        for i in range( len(labels)):\n",
        "            if labels [i] == 0:\n",
        "              neg_count += 1\n",
        "            else:\n",
        "               pos_count += 1\n",
        "\n",
        "        if pos_count == 0:\n",
        "               entropy_value = 0  - (neg_count /ins_num)* log((neg_count /ins_num))\n",
        "        elif neg_count == 0:\n",
        "            entropy_value = -1*((pos_count) / ins_num) * log(((pos_count) / ins_num))  - 0\n",
        "        else:\n",
        "            entropy_value = -1*((pos_count) / ins_num) * log(((pos_count) / ins_num))  - (neg_count /ins_num)* log((neg_count /ins_num))\n",
        "        return entropy_value\n",
        "\n",
        "\n",
        "    def unique_params(self,dataset,labels,attribute):\n",
        "\n",
        "\n",
        "        l = []\n",
        "\n",
        "        for k in range(len(labels)):\n",
        "            l.append(dataset[k][attribute])\n",
        "\n",
        "        np.array(l)\n",
        "\n",
        "        l = np.unique(l)\n",
        "\n",
        "        return l\n",
        "\n",
        "\n",
        "\n",
        "    def calculate_average_entropy__(self, dataset, labels, attribute):\n",
        "        \"\"\"\n",
        "        :param dataset: array of the data instances on which an average entropy value is calculated\n",
        "        :param labels: array of the labels of those data instances\n",
        "        :param attribute: for which attribute an average entropy value is going to be calculated...\n",
        "        :return: the calculated average entropy value for the given attribute\n",
        "        \"\"\"\n",
        "        ins, dim = np.array(dataset).shape\n",
        "        average_entropy = 0.0\n",
        "\n",
        "\n",
        "        l = []\n",
        "        for k in range(len(labels)):\n",
        "            l.append(dataset[k][attribute])\n",
        "\n",
        "        np.array(l)\n",
        "\n",
        "        l = np.unique(l)\n",
        "        #print(l)\n",
        "\n",
        "\n",
        "        entropy_array = np.zeros(shape = np.shape(l))\n",
        "        prob_array =  np.zeros(shape = np.shape(l))\n",
        "\n",
        "\n",
        "        for i in range (len(l)):\n",
        "          count_pos = 0\n",
        "          count_neg = 0\n",
        "          for k in range(len(labels)):\n",
        "            if dataset[k][attribute] == l[i] and labels [k] == 0:\n",
        "              count_neg+=1\n",
        "            elif dataset[k][attribute] == l[i] and labels [k] == 1:\n",
        "              count_pos+=1\n",
        "            #print(\"count neg is: %d, count pos is: %d and feature is %s\" %(count_neg, count_pos, l[i]))\n",
        "          if count_pos == 0 or count_neg ==0 :\n",
        "            entropy_array[i] = 0\n",
        "          else:\n",
        "            entropy_array[i] = (-1* (count_pos)/(count_pos+count_neg) * log((count_pos)/(count_pos+count_neg) ))  -  ((count_neg)/(count_pos+count_neg) * log((count_neg)/(count_pos+count_neg)))\n",
        "          prob_array [i] =  (count_pos + count_neg) / (len(labels))\n",
        "\n",
        "        #print(entropy_array)\n",
        "        #print(prob_array)\n",
        "\n",
        "\n",
        "\n",
        "        for i in range(len(entropy_array)):\n",
        "          average_entropy += prob_array[i] * entropy_array[i]\n",
        "\n",
        "\n",
        "        return average_entropy\n",
        "\n",
        "\n",
        "    def calculate_information_gain__(self, dataset, labels, attribute):\n",
        "        \"\"\"\n",
        "        :param dataset: array of the data instances on which an information gain score is going to be calculated\n",
        "        :param labels: array of the labels of those data instances\n",
        "        :param attribute: for which attribute the information gain score is going to be calculated...\n",
        "        :return: the calculated information gain score\n",
        "        \"\"\"\n",
        "        information_gain = 0.0\n",
        "        information_gain = self.calculate_entropy__(dataset,labels) - self.calculate_average_entropy__(dataset,labels, attribute)\n",
        "\n",
        "        #print(\"for attribute: %d\" %(attribute))\n",
        "        #print(self.calculate_entropy__(dataset,labels))\n",
        "        #print(self.calculate_average_entropy__(dataset,labels, attribute) )\n",
        "        return information_gain\n",
        "\n",
        "\n",
        "    def calculate_intrinsic_information__(self, dataset, labels, attribute):\n",
        "        \"\"\"\n",
        "        :param dataset: array of data instances on which an intrinsic information score is going to be calculated\n",
        "        :param labels: array of the labels of those data instances\n",
        "        :param attribute: for which attribute the intrinsic information score is going to be calculated...\n",
        "        :return: the calculated intrinsic information score\n",
        "        \"\"\"\n",
        "        intrinsic_info = None\n",
        "\n",
        "\n",
        "        ins, dim = dataset.shape()\n",
        "\n",
        "        intrinsic_info= 0.0\n",
        "\n",
        "        l = []\n",
        "        for k in range(len(labels)):\n",
        "            l.append(dataset[k][attribute])\n",
        "\n",
        "        np.array(l)\n",
        "\n",
        "        l = np.unique(l)\n",
        "\n",
        "        pos_array = np.zeros(shape = np.shape(l))\n",
        "        neg_array =  np.zeros(shape = np.shape(l))\n",
        "\n",
        "\n",
        "        for i in range (len(l)):\n",
        "          count_pos = 0\n",
        "          count_neg = 0\n",
        "          for k in range(len(labels)):\n",
        "            if  dataset[k][attribute] == l[i]  and labels[k] == 0:\n",
        "              count_neg+=1;\n",
        "            else:\n",
        "              count_pos+=1;\n",
        "          pos_array[i] = count_pos\n",
        "          neg_array[i] = count_neg\n",
        "\n",
        "        for i in range(len(l)):\n",
        "          intrinsic_info += -1 * (  (pos_array[i]+neg_array[i]) /(len(labels)) * log( (pos_array[i]+neg_array[i])/(len(labels)) ) )\n",
        "\n",
        "\n",
        "        return intrinsic_info\n",
        "\n",
        "    def calculate_gain_ratio__(self, dataset, labels, attribute):\n",
        "\n",
        "\n",
        "        return self.calculate_information_gain__(dataset,labels,attribute) / self.calculate_intrinsic_information__(dataset,labels,attribute)\n",
        "\n",
        "\n",
        "\n",
        "    def split_dataset(self, dataset, labels, attribute, param):\n",
        "\n",
        "\n",
        "       sub_dataset_0 = []\n",
        "       sub_dataset_1 = []\n",
        "       sub_labels_0 = []\n",
        "       sub_labels_1 = []\n",
        "\n",
        "       for i in range(len(labels)):\n",
        "          if dataset[i][attribute] == param :\n",
        "            sub_dataset_0.append(dataset[i] )\n",
        "            sub_labels_0.append(labels[i])\n",
        "          else:\n",
        "            sub_dataset_1.append(dataset[i])\n",
        "            sub_labels_1.append(labels[i])\n",
        "\n",
        "\n",
        "       return  sub_dataset_0,  sub_labels_0 , sub_dataset_1, sub_labels_1\n",
        "\n",
        "    def ID3__(self, dataset, labels, used_attributes, features):\n",
        "        \"\"\"\n",
        "        Recursive function for ID3 algorithm\n",
        "        :param dataset: data instances falling under the current tree node\n",
        "        :param labels: labels of those instances\n",
        "        :param used_attributes: while recursively constructing the tree, already used labels should be stored in used_attributes\n",
        "        :return: it returns a created non-leaf node or a created leaf node\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "            Your implementation\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        #l = []\n",
        "        used_features_indexes = 0\n",
        "        #find entropy of current dataset\n",
        "        #find information gain for current dataset (all features)\n",
        "        #then select the attribute with highest info gain\n",
        "        #then split dataset based on the feature and its param selected and repeat the entire calculations\n",
        "\n",
        "        if self.criterion == \"information gain\":\n",
        "\n",
        "          if len(np.unique(labels)) == 1:\n",
        "            return TreeLeafNode(dataset,labels)\n",
        "\n",
        "          else:\n",
        "             #selected feature shouldnt be an already used feature\n",
        "\n",
        "\n",
        "             feature_info_gain = [self.calculate_information_gain__(dataset,labels,feature)  for feature in range(len(features))]\n",
        "\n",
        "             used_feature_index = np.argmax(np.array(feature_info_gain))\n",
        "\n",
        "             l = self.unique_params(dataset,labels,used_feature_index)\n",
        "             print(l)\n",
        "             A = TreeNode(features[np.argmax(np.array(feature_info_gain))])\n",
        "\n",
        "             print(A.attribute)\n",
        "\n",
        "             if len(used_attributes) == 0:\n",
        "               self.root = A\n",
        "\n",
        "             used_attributes.append(features[np.argmax(np.array(feature_info_gain))])\n",
        "\n",
        "             for i in range(len(l)):\n",
        "                split_set, split_label, split_set_rem, split_label_rem = self.split_dataset(dataset,labels,np.argmax(np.array(feature_info_gain)), l[i])\n",
        "                #print(split_set)\n",
        "                A.subtrees[i]= self.ID3__(split_set, split_label, used_attributes, features)\n",
        "\n",
        "             return A\n",
        "\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"\n",
        "        :param x: a data instance, 1 dimensional Python array\n",
        "        :return: predicted label of x\n",
        "\n",
        "        If a leaf node contains multiple labels in it, the majority label should be returned as the predicted label\n",
        "        \"\"\"\n",
        "        predicted_label = None\n",
        "        \"\"\"\n",
        "            Your implementation\n",
        "        \"\"\"\n",
        "\n",
        "        Treenode= self.root\n",
        "\n",
        "        while type(Treenode)!= TreeLeafNode :\n",
        "\n",
        "          #if Treenode == self.root:\n",
        "             bool_array = [ Treenode.attribute == features[i]  for i in  range(len(features)) ]\n",
        "             index = np.where(bool_array)[0][0]\n",
        "             l = self.unique_params(self.dataset,self.labels,index)\n",
        "             #l indexed represent the keys of subtrees of the root\n",
        "             #extract the param at feature index\n",
        "             instance_param = x[index]\n",
        "\n",
        "             bool_array2 = [ instance_param == l[i]  for i in  range(len(l)) ]\n",
        "             subtree_key = np.where(bool_array2)[0][0]\n",
        "             Treenode = Treenode.subtrees[subtree_key]\n",
        "\n",
        "        if type(Treenode) == TreeLeafNode :\n",
        "          predicted_label = Treenode.labels[0]\n",
        "\n",
        "\n",
        "        return predicted_label\n",
        "\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        self.root = self.ID3__(self.dataset, self.labels, [], self.features)\n",
        "\n",
        "        print(\"Training completed\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "#from ID3 import DecisionTree\n",
        "\n",
        "\n",
        "features = [\"Temperature\", \"Outlook\", \"Humidity\", \"Windy\"]\n",
        "# Golf played?...\n",
        "labels = [0,0,1,1,1,0,1,1,1,1,1,0,0,1]\n",
        "dataset = [\n",
        "[\"hot\" ,\"sunny\", \"high\", \"false\"],\n",
        "[\"hot\" ,\"sunny\", \"high\", \"true\"],\n",
        "[\"hot\" ,\"overcast\", \"high\", \"false\"],\n",
        "[\"cool\", \"rain\", \"normal\", \"false\"],\n",
        "[\"cool\", \"overcast\", \"normal\", \"true\"],\n",
        "[\"mild\", \"sunny\", \"high\", \"false\"],\n",
        "[\"cool\", \"sunny\", \"normal\", \"false\"],\n",
        "[\"mild\", \"rain\", \"normal\", \"false\"],\n",
        "[\"mild\", \"sunny\", \"normal\", \"true\"],\n",
        "[\"mild\", \"overcast\", \"high\", \"true\"],\n",
        "[\"hot\" ,\"overcast\", \"normal\", \"false\"],\n",
        "[\"mild\", \"rain\", \"high\", \"true\"],\n",
        "[\"cool\", \"rain\", \"normal\", \"true\"],\n",
        "[\"mild\", \"rain\", \"high\", \"false\"]]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "tree= DecisionTree(dataset, labels, features, \"information gain\" )\n",
        "\n",
        "tree.train()\n",
        "\n",
        "\n",
        "print(tree.root.attribute)\n",
        "\n",
        "for i in range(len(tree.root.subtrees)):\n",
        "\n",
        "  if type(tree.root.subtrees[i]) == TreeNode   :\n",
        "   print(tree.root.subtrees[i].attribute)\n",
        "   for k in range(len(tree.root.subtrees[i].subtrees)):\n",
        "     if type(tree.root.subtrees[i].subtrees[k]) == TreeNode:\n",
        "       print(tree.root.subtrees[i].subtrees[k].attribute)\n",
        "     else:\n",
        "          print(tree.root.subtrees[i].subtrees[k].labels)\n",
        "          print(\"\\n\")\n",
        "\n",
        "  else:\n",
        "     print(tree.root.subtrees[i].labels)\n",
        "     print(\"\\n\")\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#print( np.where([\"Temperature\" == features[i]  for i in  range(len(features)) ])[0][0] )\n",
        "\n",
        "\n",
        "\n",
        "dt = DecisionTree(dataset, labels, features)\n",
        "dt.train()\n",
        "correct = 0\n",
        "wrong = 0\n",
        "for data_index in range(len(dataset)):\n",
        "    data_point = dataset[data_index]\n",
        "    data_label = labels[data_index]\n",
        "\n",
        "    predicted = dt.predict(data_point)\n",
        "    if predicted == data_label:\n",
        "        correct += 1\n",
        "    else:\n",
        "        wrong += 1\n",
        "\n",
        "\n",
        "print(\"Accuracy : %.2f\" % (correct/(correct+wrong)*100))\n",
        "\n",
        "#print(tree.root.subtrees[1].subtrees[0].labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLoBgJQ46iU7",
        "outputId": "a305e913-e928-4bcb-bc8c-f80eb89b57dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['overcast' 'rain' 'sunny']\n",
            "Outlook\n",
            "['false' 'true']\n",
            "Windy\n",
            "['high' 'normal']\n",
            "Humidity\n",
            "Training completed\n",
            "Accuracy : 100.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uxBpyHYQSYEJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}